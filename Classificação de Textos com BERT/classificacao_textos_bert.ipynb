{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatashaB-randao/NLP/blob/main/classificacao_textos_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BuUfk9qUdDs"
      },
      "source": [
        "Este notebook apresenta o desenvolvimento de um projeto de classificação de textos utilizando o modelo BERT. O processo é estruturado e documentado, abrangendo todas as etapas essenciais, desde o carregamento e pré-processamento dos dados até o treinamento, ajuste fino e avaliação do modelo. Um pipeline completo é implementado para garantir a reprodutibilidade e a eficiência do experimento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F0Fm5L_Ut0X"
      },
      "source": [
        "### Projeto: Classificação de Textos com BERT\n",
        "Este projeto utiliza o modelo BERT pré-treinado para classificar textos do dataset AG News em quatro categorias:\n",
        "\n",
        "- World (Notícias sobre o mundo)\n",
        "- Sports (Esportes)\n",
        "- Business (Negócios)\n",
        "- Sci/Tech (Ciência/Tecnologia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbY1tiNsUwkT"
      },
      "source": [
        "### Instalação das Dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fouPx5XiMtQP",
        "outputId": "a85adcc5-8b86-4501-b20a-af42916ccd1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets scikit-learn torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trK6P2vhTbTp"
      },
      "source": [
        "### Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3EVtDqppS20z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\natas\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                        Trainer, TrainingArguments, pipeline)\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYjudHcVThbd"
      },
      "source": [
        "### Carregamento do Dataset\n",
        "\n",
        "O AG News é um dataset de notícias já etiquetadas em quatro categorias.\n",
        "Baixamos o conjunto de dados diretamente do Hugging Face Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWxNXibITibH",
        "outputId": "ee466782-72ae-4788-853a-027d3e9a9dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        }
      ],
      "source": [
        "# Carregar dataset AG News\n",
        "dataset = load_dataset('ag_news')\n",
        "\n",
        "# Exibir um exemplo do dataset\n",
        "print(dataset['train'][0])\n",
        "\n",
        "\n",
        "# OBS.: O campo text contém o conteúdo da notícia, e label indica a categoria (0 a 3).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeWJJUhxTo1g"
      },
      "source": [
        "### Pré-processamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZC6vMjb5TplI"
      },
      "outputs": [],
      "source": [
        "# Inicializando o tokenizer do BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Função de tokenização e preparação dos dados\n",
        "def preprocess_data(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Aplicando a tokenização ao dataset\n",
        "tokenized_datasets = dataset.map(preprocess_data, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaM2UHgnTtHS"
      },
      "source": [
        "### Carregamento do Modelo BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDwpkcNxTu7O",
        "outputId": "056189ec-93f9-49c7-ce4c-810fbb143457"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Carregando o modelo BERT para classificação de sequência\n",
        "num_classes = 4  # Número de classes no dataset AG News\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-EHmseSTv93"
      },
      "source": [
        "### Preparação para o Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kpAKUjTHT4t0"
      },
      "outputs": [],
      "source": [
        "# Definindo os argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Diretório para salvar os resultados\n",
        "    num_train_epochs=3,              # Número de épocas de treinamento\n",
        "    per_device_train_batch_size=16,  # Tamanho do lote de treinamento\n",
        "    per_device_eval_batch_size=64,   # Tamanho do lote de avaliação\n",
        "    warmup_steps=500,                 # Número de passos para aquecimento\n",
        "    weight_decay=0.01,               # Taxa de decaimento de peso\n",
        "    logging_dir='./logs',            # Diretório para logs\n",
        "    logging_steps=10,                 # Frequência de logs\n",
        "    report_to=\"none\",                # Desativar W&B se não for necessário\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bvc3FDeXEX8"
      },
      "source": [
        "### Criando o Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "peD6ryNIXGFk"
      },
      "outputs": [],
      "source": [
        "# Criando o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNFAMgllU5Ui"
      },
      "source": [
        "### Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d561NqYcU8XI"
      },
      "outputs": [],
      "source": [
        "# Treinando o modelo\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot6nNEzVXu5S"
      },
      "source": [
        "### Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW8aKo5SXye1"
      },
      "outputs": [],
      "source": [
        "# Avaliando o modelo no conjunto de teste\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(evaluation_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BVa8tCMXrPs"
      },
      "source": [
        "### Teste do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b96Ekq-uX1g3"
      },
      "outputs": [],
      "source": [
        "# Função para realizar previsões em um texto específico\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return predictions.item()\n",
        "\n",
        "# Testando o modelo com um exemplo\n",
        "test_text = \"This is an example news article about technology.\"\n",
        "predicted_class = predict(test_text)\n",
        "print(f'Predição para o texto: {predicted_class}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcwcG9fcX3-9"
      },
      "source": [
        "### Fazer Previsões em um Conjunto de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vugJlUwmX9va"
      },
      "outputs": [],
      "source": [
        "# Fazendo previsões em um conjunto de dados\n",
        "test_samples = [\"Breaking news: Scientists discover new planet.\",\n",
        "                \"The economy is experiencing a downturn.\"]\n",
        "\n",
        "predictions = [predict(sample) for sample in test_samples]\n",
        "for sample, pred in zip(test_samples, predictions):\n",
        "    print(f'Texto: \"{sample}\" - Predição: {pred}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMF/m+5bHR3lPJvF49jEruw",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
